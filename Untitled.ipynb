{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isuf\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty sentences : 0\n",
      "data loaded!\n",
      "         number of documents: 85796\n",
      "         vocab size: 422329\n",
      "         max document length: 250\n",
      "         avg document length: 71.0850972073\n",
      "         min document length: 1\n",
      "         max char length: 32761\n",
      "         avg char length: 419.945486969\n",
      "         min char length: 3\n",
      "Extracting tf features for LDA...\n",
      "Fitting LDA models with tf features\n",
      "     done in 6.231s.\n",
      "training the model\n",
      "   done in 774.591s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "gt lt legends bot user login download pass amp http\n",
      "Topic #1:\n",
      "use script banned just game did working rat work try\n",
      "Topic #2:\n",
      "file download com www http virustotal hide analysis amp https\n",
      "Topic #3:\n",
      "spoiler added new fixed bot version clashbot features download attack\n",
      "Topic #4:\n",
      "com gmail hotmail yahoo hide net aol uk live fr\n",
      "Topic #5:\n",
      "hide com https http www download bol nulled cracked file\n",
      "Topic #6:\n",
      "just like use need account thanks want password accounts make\n",
      "Topic #7:\n",
      "hide 123456a 123456789a abc123 qwerty123 1q2w3e4r qwerty1 lol123 password1 qwe123\n",
      "Topic #8:\n",
      "account accounts email http com pm buy paypal skins euw\n",
      "Topic #9:\n",
      "hide game sims battlefield games edition em fifa dragon eaid\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isuf\\appdata\\local\\programs\\python\\python36\\lib\\json\\encoder.py:199: DeprecationWarning: Interpreting naive datetime as local 2017-05-06 20:44:56.969307. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import Utils as Utils\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import pickle \n",
    "import Configuration\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    '''Prints the n top words from the model.\n",
    "\n",
    "    :param model: The model\n",
    "    :param feature_names: The feature names\n",
    "    :param n_top_words: Number of top words\n",
    "    '''\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "def run_LDA(corpus,num_topics=10,maximum_iterations=10, learn_offfset=10.0):\n",
    "      \n",
    "    t0 = time()\n",
    "\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, \n",
    "                                    min_df=2, \n",
    "                                    stop_words='english', \n",
    "                                    analyzer='word')\n",
    "\n",
    "    print(\"Fitting LDA models with tf features\")\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    print(\"     done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    #2. Define the model\n",
    "    lda = LatentDirichletAllocation(n_topics=num_topics, \n",
    "                                    learning_method='online',\n",
    "                                    max_iter=maximum_iterations,\n",
    "                                    learning_offset=learn_offfset,\n",
    "                                    random_state=0)\n",
    "\n",
    "    # 3. Train the model\n",
    "    t0 = time()\n",
    "    print(\"training the model\")\n",
    "    lda.fit(tf)\n",
    "    print(\"   done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "     # Print LDA model\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    print_top_words(lda, tf_feature_names, n_top_words=10)\n",
    "\n",
    "    return lda, tf,tf_vectorizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = Configuration.Parameters()\n",
    "    file_locations=params.file_locations\n",
    "    param = params.param\n",
    "  \n",
    "    clean_string = True\n",
    "    remove_stop_words=False\n",
    "    retrain=True\n",
    "\n",
    "    dataset=\"Security_data\"\n",
    "    n_topics=10\n",
    "\n",
    "    data_folder_security=[\"results\\char_trigram_binary\\security_data.txt\"] \n",
    "    #[params.baseline_locations[\"tmp\"][0]]# [\"results\\Security_data.txt\"] #[params.baseline_locations[\"Nulled_Binary\"][1]]\n",
    "    docs, vocab,_ = Utils.load(data_folder_security,clean_string=clean_string,remove_stop_words=remove_stop_words,split_for_cv=True)\n",
    "    max_l= Utils.get_dataset_details(docs,vocab)\n",
    "    data = [ sent[\"text\"] for sent in docs]\n",
    "    \n",
    "    if(retrain):\n",
    "        lda, tf,tf_vectorizer=run_LDA(data,num_topics=n_topics,maximum_iterations=10)\n",
    "        #Utils.save_pickle([lda, tf,tf_vectorizer],dataset+str(n_topics)+\"_topics.pkl\")\n",
    "    else:\n",
    "        x=Utils.load_pickle(\"lda_models\\\\\"+dataset+str(n_topics)+\"_topics.pkl\")\n",
    "        lda, tf,tf_vectorizer=x[0],x[1],x[2]\n",
    "        \n",
    "    pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
