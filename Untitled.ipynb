{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty sentences : 0\n",
      "data loaded!\n",
      "         number of documents: 9999\n",
      "         vocab size: 129284\n",
      "         max document length: 250\n",
      "         avg document length: 61.095209521\n",
      "         min document length: 1\n",
      "         max char length: 6752\n",
      "         avg char length: 396.772277228\n",
      "         min char length: 1\n",
      "Running Topic Models...\n",
      "          Number of Topics : 30\n",
      "          Number of Words per Topics : 10\n",
      "          Maximum  Number of Features : 50000\n",
      "Extracting tf features for LDA...\n",
      "Fitting LDA models with tf features, n_samples=9999 and n_features=50000...\n",
      "     done in 0.829s.\n",
      "training the model\n",
      "   done in 53.805s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "              evaluate_every=-1, learning_decay=0.7,\n",
       "              learning_method='online', learning_offset=10.0,\n",
       "              max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "              n_jobs=1, n_topics=30, perp_tol=0.1, random_state=0,\n",
       "              topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       " <9999x50000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 224633 stored elements in Compressed Sparse Row format>,\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.75, max_features=50000, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from time import time\n",
    "import Utils as Utils\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn \n",
    "pyLDAvis.enable_notebook()\n",
    "import pickle \n",
    "\n",
    "import Configuration \n",
    "\n",
    "n_samples = 10000\n",
    "def print_topics (lda_model, tf_vectorizer, num_words_per_topic, store_topics=False):\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        topic_no =\"Topic #%d:\" % topic_idx\n",
    "        topic_kw = \" \".join([tf_feature_names[i] for i in topic.argsort()[:-num_words_per_topic - 1:-1]])\n",
    "        print(topic_no)\n",
    "        print(''.join([tf_feature_names[i] + ' ' + str(round(topic[i]/ sum(topic),4 ))  +' | ' for i in topic.argsort()[:-num_words_per_topic - 1:-1]]))\n",
    "        #print(topic_kw)\n",
    "        print()\n",
    "        if(store_topics==True):\n",
    "          Utils.write_to_file(\"topics.txt\",topic_no+\"\\n\",\"a\")\n",
    "          Utils.write_to_file(\"topics.txt\",topic_kw+\"\\n\",\"a\")\n",
    "\n",
    "def print_lda_parameters(num_topics=10, num_top_words=10, num_features=30000,heldout_data_size =0.33):\n",
    "    print(\"Running Topic Models...\")\n",
    "    print(\"          Number of Topics : \" + str(num_topics))\n",
    "    print(\"          Number of Words per Topics : \" + str(num_top_words))\n",
    "    print(\"          Maximum  Number of Features : \" + str(num_features))\n",
    "\n",
    "def store_topics(lda,tf,corpus_docs):\n",
    "     ##Store the data according to their topiccs\n",
    "    print(\"Storing data to files \")\n",
    "    doc_topic= lda.transform(tf)\n",
    "    num_docs, num_topics= doc_topic.shape\n",
    "    for doc_idx in range(num_docs):\n",
    "        doc_most_pr_topic = np.argmax(doc_topic[doc_idx])\n",
    "        document_text = corpus_docs[doc_idx] \n",
    "        file_location = \"Results/topic_\" + str(doc_most_pr_topic) + \"_documents.txt\"\n",
    "        Utils.write_to_file(file_location,document_text,\"a\")\n",
    "    print(\"data stored to corrensponding files\") \n",
    "\n",
    "#def visualize_topics ():\n",
    "#    x = pickle.load(open(\"lda_model_Nulled.pkl\",\"rb\"))\n",
    "#    print(\"LDA model loaded \")\n",
    "#    lda, tf, tf_vectorizer= x[0], x[1], x[2]\n",
    "#    lda.fit(tf)\n",
    "#    pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "#    print(\"prepared...!\")\n",
    "\n",
    "############ Main method #########\n",
    "def build_LDA_model(corpus_data, num_topics=10, max_n_features =30000, max_df=0.95, min_df=1, max_iter=5):\n",
    "    \n",
    "    ''' \n",
    "         common English words are removed,\n",
    "         words occurring in min_df(e.g=1) documents are removed \n",
    "         words occurring in at least 95% of the documents are removed.\n",
    "\n",
    "    '''\n",
    "   \n",
    "    t0 = time()\n",
    "    #1  Feature Extaction from raw data. Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                    max_features=max_n_features,\n",
    "                                    stop_words='english')\n",
    "    print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\" % (len(corpus_data), max_n_features))\n",
    "    tf = tf_vectorizer.fit_transform(corpus_data)\n",
    "    print(\"     done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "    #2. Build the model\n",
    "    lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=max_iter,\n",
    "                                    learning_method='online',\n",
    "                                    #learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    \n",
    "\n",
    "    # 3. Train the model\n",
    "    t0 = time()\n",
    "    print(\"training the model\")\n",
    "    lda.fit(tf)\n",
    "    print(\"   done in %0.3fs.\" % (time() - t0))\n",
    "    pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "    return lda, tf,tf_vectorizer\n",
    "\n",
    "\n",
    "def run_LDA(corpus_data, evaluate=False,evaluation_data=[],  num_topics=10, n_top_words=10, max_n_features =30000, max_df=0.95, min_df=1, max_iter=5,\n",
    "            show_topics=True,store=True):\n",
    "    print_lda_parameters(num_topics, n_top_words, max_n_features)\n",
    "\n",
    "    # 1. Preprocessing the data \n",
    "\n",
    "    # 2. Build the Model \n",
    "    lda,tf,tf_vectorizer=build_LDA_model(corpus_data,\n",
    "                                         num_topics, max_n_features,\n",
    "                                         max_df, min_df,max_iter)\n",
    "    \n",
    "    pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "    pickle.dump([lda,tf,tf_vectorizer ], open(\"lda_model_Nulled.pkl\", \"wb\"))\n",
    "    return lda,tf,tf_vectorizer \n",
    "\n",
    "params = Configuration.Parameters()\n",
    "file_locations=params.file_locations\n",
    "param = params.param\n",
    "\n",
    "data_folder_security=params.baseline_locations[\"tmp\"]#[\"results\\Security_data.txt\"] #[params.baseline_locations[\"Nulled_Binary\"][1]]\n",
    "docs, vocab = Utils.load(data_folder_security,clean_string=True,remove_stop_words=False,split_for_cv=True)\n",
    "max_l= Utils.get_dataset_details(docs,vocab)\n",
    "labels = [sent[\"y\"] for sent in docs]\n",
    "data = [ sent[\"text\"] for sent in docs]\n",
    "\n",
    "run_LDA(data,evaluate=False,evaluation_data=[],\n",
    "                  num_topics=30, n_top_words=10, max_n_features=50000, \n",
    "                  max_df=0.75, min_df=1, max_iter=5,\n",
    "                  show_topics=True, store=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
